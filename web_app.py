import os
import pickle
import glob
import argparse
import streamlit as st
import pandas as pd
import numpy as np
from loguru import logger

# ëª…ë ¹ì¤„ ì¸ìˆ˜ íŒŒì„œ ì„¤ì •
parser = argparse.ArgumentParser(description='Streamlit NER ë²¤ì¹˜ë§ˆí¬ ì‹œê°í™” ë„êµ¬')
parser.add_argument('--fix-arrow-error', action='store_true', 
                    help='ë°ì´í„°í”„ë ˆì„ ì»¬ëŸ¼ì˜ í˜¼í•© íƒ€ì…ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ Arrow ë³€í™˜ ì˜¤ë¥˜ ë°©ì§€')
args = parser.parse_args()

st.set_page_config(
    page_title="LLM NER ë²¤ì¹˜ë§ˆí¬ ì‹œê°í™”", 
    page_icon="ğŸ“Š", 
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("LLM NER ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì‹œê°í™”")

# ë°ì´í„°í”„ë ˆì„ ì»¬ëŸ¼ íƒ€ì… ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_dataframe(df):
    """ë¦¬ìŠ¤íŠ¸ì™€ ë¹„ë¦¬ìŠ¤íŠ¸ê°€ í˜¼í•©ëœ ì»¬ëŸ¼ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ Arrow ë³€í™˜ ì˜¤ë¥˜ ë°©ì§€"""
    if not args.fix_arrow_error:
        return df
    
    # ë°ì´í„°í”„ë ˆì„ ë³µì‚¬
    processed_df = df.copy()
    
    # ê° ì»¬ëŸ¼ ê²€ì‚¬
    for col in processed_df.columns:
        # ìƒ˜í”Œ ë°ì´í„°ì—ì„œ ë¦¬ìŠ¤íŠ¸ íƒ€ì… ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        has_list = False
        has_non_list = False
        
        sample_values = processed_df[col].dropna().head(100).values
        for val in sample_values:
            if isinstance(val, (list, tuple, np.ndarray)):
                has_list = True
            elif not pd.isna(val):
                has_non_list = True
            
            # ë¦¬ìŠ¤íŠ¸ì™€ ë¹„ë¦¬ìŠ¤íŠ¸ê°€ ëª¨ë‘ ë°œê²¬ë˜ë©´ ë°˜ë³µ ì¢…ë£Œ
            if has_list and has_non_list:
                break
        
        # ë¦¬ìŠ¤íŠ¸ì™€ ë¹„ë¦¬ìŠ¤íŠ¸ê°€ í˜¼í•©ëœ ì»¬ëŸ¼ì€ ë¬¸ìì—´ë¡œ ë³€í™˜
        if has_list and has_non_list:
            logger.info(f"ì»¬ëŸ¼ '{col}'ì— í˜¼í•© íƒ€ì… ë°œê²¬, ë¬¸ìì—´ë¡œ ë³€í™˜ ì¤‘...")
            processed_df[col] = processed_df[col].apply(
                lambda x: str(x) if not pd.isna(x) else x
            )
    
    return processed_df

# ê²°ê³¼ ë¡œë”© í•¨ìˆ˜
def load_results():
    results_dir = "results"
    all_results = {}
    framework_model_info = {}
    
    # ê²°ê³¼ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  í•˜ìœ„ í´ë” ê²€ìƒ‰
    result_dirs = [os.path.join(results_dir, d) for d in os.listdir(results_dir) 
                  if os.path.isdir(os.path.join(results_dir, d))]
    
    # ê²°ê³¼ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ results ë””ë ‰í† ë¦¬ ìì²´ ì‚¬ìš©
    if not result_dirs:
        result_dirs = [results_dir]
    
    for result_dir in result_dirs:
        # ê° ê²°ê³¼ ë””ë ‰í† ë¦¬ì—ì„œ PKL íŒŒì¼ ê²€ìƒ‰
        pkl_files = glob.glob(os.path.join(result_dir, "*.pkl"))
        
        for file_path in pkl_files:
            try:
                with open(file_path, "rb") as file:
                    framework_results = pickle.load(file)
                    all_results.update(framework_results)
                    
                    # ëª¨ë¸ ì •ë³´ ì¶”ì¶œ
                    for key, value in framework_results.items():
                        framework_model_info[key] = {
                            "model": value.get("llm_model", "unknown"),
                            "host": value.get("llm_model_host", "unknown"),
                            "source_data": value.get("source_data_path", ""),
                            "file_path": file_path  # íŒŒì¼ ê²½ë¡œ ì €ì¥
                        }
            except Exception as e:
                st.warning(f"íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {file_path}: {str(e)}")
    
    return all_results, framework_model_info

# ë©”ì¸ í•¨ìˆ˜
def main():
    # ê²°ê³¼ ë¡œë”©
    with st.spinner("ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë¡œë”© ì¤‘..."):
        results, model_info = load_results()
    
    if not results:
        st.error("ì²˜ë¦¬í•  ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. results í´ë”ì— ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    # ì‚¬ì´ë“œë°” - ëª¨ë¸ ì„ íƒ
    st.sidebar.header("ëª¨ë¸ ì„ íƒ")
    
    # íŒŒì¼ ê²½ë¡œë¥¼ í¬í•¨í•œ ì˜µì…˜ ìƒì„±
    model_options = {}
    for key in results.keys():
        file_path = model_info.get(key, {}).get("file_path", "unknown")
        display_name = f"{os.path.basename(os.path.dirname(file_path))}/{os.path.basename(file_path)} - {key}"
        model_options[display_name] = key
    
    selected_display_names = st.sidebar.multiselect(
        "ë¹„êµí•  í”„ë ˆì„ì›Œí¬/ëª¨ë¸ ì„ íƒ",
        options=list(model_options.keys()),
        default=list(model_options.keys())[:min(3, len(model_options.keys()))]
    )
    
    # ì„ íƒëœ í‘œì‹œ ì´ë¦„ì„ ì‹¤ì œ í‚¤ë¡œ ë³€í™˜
    selected_frameworks = [model_options[name] for name in selected_display_names]
    
    # ëª¨ë¸ ì •ë³´ í‘œì‹œ
    st.header("ëª¨ë¸ ì •ë³´")
    model_info_df = pd.DataFrame({
        "Framework": [f for f in selected_frameworks],
        "Model": [model_info.get(f, {}).get("model", "unknown") for f in selected_frameworks],
        "Host": [model_info.get(f, {}).get("host", "unknown") for f in selected_frameworks]
    })
    
    # Arrow ë³€í™˜ ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬
    model_info_df = preprocess_dataframe(model_info_df)
    st.dataframe(model_info_df)
    
    if not selected_frameworks:
        st.warning("ë¹„êµí•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.")
        return
    
    # ì†ŒìŠ¤ ë°ì´í„° ë¡œë“œ
    st.header("ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ")
    
    # ì†ŒìŠ¤ ë°ì´í„° ê²½ë¡œ ì„ íƒ
    source_paths = {model_info.get(f, {}).get("source_data", "") for f in selected_frameworks}
    source_paths = [p for p in source_paths if p]
    
    if not source_paths:
        st.error("ì†ŒìŠ¤ ë°ì´í„° ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    source_path = st.selectbox("ì†ŒìŠ¤ ë°ì´í„° ì„ íƒ", options=source_paths)
    
    try:
        source_data = pd.read_pickle(source_path)
        ground_truths = source_data["labels"].tolist()
        texts = source_data["text"].tolist()
    except Exception as e:
        st.error(f"ì†ŒìŠ¤ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return
    
    # ìƒ˜í”Œ ì„ íƒ
    sample_idx = st.slider("ìƒ˜í”Œ ì¸ë±ìŠ¤ ì„ íƒ", 0, len(texts), 0)
    
    # í˜„ì¬ ìƒ˜í”Œì˜ Ground Truthì™€ ì˜ˆì¸¡ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    ground_truth = ground_truths[sample_idx]
    
    # ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
    model_predictions = {}
    
    for framework in selected_frameworks:
        try:
            predictions = results[framework]["predictions"][sample_idx]
            # ì—¬ëŸ¬ ì‹¤í–‰ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš° ì²« ë²ˆì§¸ ê²°ê³¼ë§Œ ì‚¬ìš©
            if isinstance(predictions, list) and len(predictions) > 0:
                model_predictions[framework] = predictions[0]
            else:
                model_predictions[framework] = predictions
        except (IndexError, KeyError) as e:
            model_predictions[framework] = {}
    
    # Ground Truthì™€ ëª¨ë“  ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ì—ì„œ ëª¨ë“  ì¹´í…Œê³ ë¦¬(í‚¤) ìˆ˜ì§‘
    all_categories = set()
    all_categories.update(ground_truth.keys())
    for framework, pred in model_predictions.items():
        # ì˜ˆì¸¡ ê²°ê³¼ê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°ì—ë§Œ keys() í˜¸ì¶œ
        if isinstance(pred, dict):
            all_categories.update(pred.keys())
        elif isinstance(pred, list):
            # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ê° í•­ëª©ì´ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
            st.warning(f"{framework}ì˜ ì˜ˆì¸¡ ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì…ë‹ˆë‹¤. ì²« ë²ˆì§¸ í•­ëª©ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            if pred and isinstance(pred[0], dict):
                all_categories.update(pred[0].keys())
        else:
            st.warning(f"{framework}ì˜ ì˜ˆì¸¡ ê²°ê³¼ íƒ€ì…({type(pred)})ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤.")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ê²°ê³¼ í‘œì‹œ
    st.subheader("ì¹´í…Œê³ ë¦¬ë³„ ë¼ë²¨ ë¹„êµ")
    
    # ëª¨ë“  ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ í‘œ ìƒì„±
    for category in sorted(all_categories):
        st.write(f"**ì¹´í…Œê³ ë¦¬: {category}**")
        
        # Ground Truthì˜ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ì—”í‹°í‹°
        gt_entities = set(ground_truth.get(category, []))
        
        # ëª¨ë“  ëª¨ë¸ê³¼ Ground Truthë¥¼ í¬í•¨í•œ í‘œ ë°ì´í„°
        comparison_data = []
        
        # Ground Truth ë°ì´í„° ì¶”ê°€
        comparison_data.append({
            "ëª¨ë¸": "Ground Truth",
            "ì—”í‹°í‹°": ", ".join(gt_entities) if gt_entities else "(ì—†ìŒ)"
        })
        
        # ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ ì¶”ê°€
        for framework, prediction in model_predictions.items():
            # ì˜ˆì¸¡ì´ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
            if not isinstance(prediction, dict):
                comparison_data.append({
                    "ëª¨ë¸": framework,
                    "ì—”í‹°í‹°": f"(ì˜ëª»ëœ í˜•ì‹: {type(prediction)})",
                    "ì •í™•íˆ ë§ì¶˜ ê²ƒ(TP)": "(ì—†ìŒ)",
                    "ì˜ëª» ì˜ˆì¸¡(FP)": "(ì—†ìŒ)",
                    "ë†“ì¹œ ê²ƒ(FN)": ", ".join(gt_entities) if gt_entities else "(ì—†ìŒ)"
                })
                continue
                
            pred_entities = set(prediction.get(category, []))
            
            # TP, FP, FN ê³„ì‚° (metrics.pyì™€ ë™ì¼í•œ ë°©ì‹)
            true_positives = pred_entities.intersection(gt_entities)
            false_positives = pred_entities - gt_entities
            false_negatives = gt_entities - pred_entities
            
            # ë°ì´í„° ì¶”ê°€
            comparison_data.append({
                "ëª¨ë¸": framework,
                "ì—”í‹°í‹°": ", ".join(pred_entities) if pred_entities else "(ì—†ìŒ)",
                "ì •í™•íˆ ë§ì¶˜ ê²ƒ(TP)": ", ".join(true_positives) if true_positives else "(ì—†ìŒ)",
                "ì˜ëª» ì˜ˆì¸¡(FP)": ", ".join(false_positives) if false_positives else "(ì—†ìŒ)",
                "ë†“ì¹œ ê²ƒ(FN)": ", ".join(false_negatives) if false_negatives else "(ì—†ìŒ)"
            })
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        comparison_df = pd.DataFrame(comparison_data)
        
        # Arrow ë³€í™˜ ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬
        comparison_df = preprocess_dataframe(comparison_df)
        
        # í‘œ í‘œì‹œ
        st.dataframe(comparison_df, use_container_width=True)
    
    # ëª¨ë¸ë³„ ì „ì²´ ì˜ˆì¸¡ ê²°ê³¼ í‘œì‹œ (ì ‘ì„ ìˆ˜ ìˆëŠ” ì˜ì—­)
    with st.expander("ì „ì²´ ì˜ˆì¸¡ ê²°ê³¼ ë³´ê¸°"):
        for framework in selected_frameworks:
            st.write(f"**{framework}**")
            try:
                predictions = results[framework]["predictions"][sample_idx]
                
                # ì—¬ëŸ¬ ì‹¤í–‰ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°
                if isinstance(predictions, list) and len(predictions) > 0:
                    for i, pred in enumerate(predictions):
                        st.write(f"Run {i+1}:")
                        st.json(pred)
                else:
                    st.json(predictions)
            except (IndexError, KeyError) as e:
                st.write(f"í•´ë‹¹ ìƒ˜í”Œì— ëŒ€í•œ ì˜ˆì¸¡ ê²°ê³¼ ì—†ìŒ ({str(e)})")

    # ë©”íŠ¸ë¦­ ë¹„êµ
    st.header("ì„±ëŠ¥ ë©”íŠ¸ë¦­")
    metrics_data = {}
    
    for framework in selected_frameworks:
        try:
            latencies = results[framework]["latencies"][sample_idx]
            success_rate = results[framework]["percent_successful"][sample_idx]
            
            metrics_data[framework] = {
                "í‰ê·  ì§€ì—°ì‹œê°„(ì´ˆ)": sum(latencies) / len(latencies) if latencies else 0,
                "ì„±ê³µë¥ (%)": success_rate * 100 if isinstance(success_rate, (int, float)) else 0
            }
        except (IndexError, KeyError, ZeroDivisionError) as e:
            metrics_data[framework] = {
                "í‰ê·  ì§€ì—°ì‹œê°„(ì´ˆ)": 0,
                "ì„±ê³µë¥ (%)": 0
            }
            st.warning(f"{framework}ì˜ ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    metrics_df = pd.DataFrame(metrics_data).T
    
    # Arrow ë³€í™˜ ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬
    metrics_df = preprocess_dataframe(metrics_df)
    st.dataframe(metrics_df)

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        st.error(f"ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        logger.exception("Streamlit ì•± ì‹¤í–‰ ì˜¤ë¥˜")
